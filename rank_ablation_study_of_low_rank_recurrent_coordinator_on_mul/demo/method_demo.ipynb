{
 "cells": [
  {
   "cell_type": "code",
   "source": "def run_experiment(examples: List[Dict], coordinator, tracker: TokenTracker, name: str) -> Tuple[List[str], List[int]]:\n    \"\"\"Run experiment with given coordinator.\"\"\"\n    print(f\"\\\\n{BLUE}Running experiment: {name}{END}\")\n\n    predictions = []\n    token_counts = []\n\n    for idx, example in enumerate(examples):\n        coordinator.reset()\n        episode_start_tokens = tracker.total_tokens\n\n        context = example['context']\n        agent_outputs = [context['response_a'], context['response_b']]\n\n        state, coordinator_message = coordinator.step(agent_outputs)\n        step_tokens = tracker.log_coordinator_step(agent_outputs, coordinator_message)\n\n        prediction = predict_winner(state, agent_outputs, context)\n        predictions.append(prediction)\n\n        episode_tokens = tracker.total_tokens - episode_start_tokens\n        token_counts.append(episode_tokens)\n\n    print(f\"{GREEN}Experiment {name} complete: {len(predictions)} predictions{END}\")\n    return predictions, token_counts\n\n\ndef predict_winner(state: np.ndarray, agent_outputs: List[str], context: Dict) -> str:\n    \"\"\"Predict winner based on coordinator state.\"\"\"\n    len_a = len(agent_outputs[0])\n    len_b = len(agent_outputs[1])\n\n    confidence = np.sum(state[state > 0])\n\n    score_a = len_a * (1 + confidence * 0.1)\n    score_b = len_b * (1 + confidence * 0.1)\n\n    if abs(score_a - score_b) < 50:\n        return \"tie\"\n    elif score_a > score_b:\n        return \"model_a\"\n    else:\n        return \"model_b\"\n\n\ndef evaluate_performance(predictions: List[str], ground_truth: List[str]) -> Dict[str, float]:\n    \"\"\"Evaluate task performance metrics.\"\"\"\n    accuracy = accuracy_score(ground_truth, predictions)\n    f1_macro = f1_score(ground_truth, predictions, average='macro', zero_division=0)\n    f1_weighted = f1_score(ground_truth, predictions, average='weighted', zero_division=0)\n\n    return {\n        \"accuracy\": float(accuracy),\n        \"f1_macro\": float(f1_macro),\n        \"f1_weighted\": float(f1_weighted)\n    }\n\nprint(\"üî¨ Experiment framework functions implemented!\")\nprint(\"   - run_experiment: Executes coordinator on dataset\")\nprint(\"   - predict_winner: Makes predictions based on coordinator state\") \nprint(\"   - evaluate_performance: Calculates accuracy and F1 scores\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üî¨ Experiment Framework\n\nLet's implement the core experiment functions for running the rank ablation study.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class FullRankCoordinator:\n    \"\"\"Baseline: Full-rank recurrent coordinator for multi-agent coordination.\"\"\"\n\n    def __init__(self, hidden_dim: int = 256):\n        \"\"\"Initialize full-rank coordinator.\"\"\"\n        self.hidden_dim = hidden_dim\n        self.W = np.random.randn(hidden_dim, hidden_dim) * 0.01\n        self.state = np.zeros(hidden_dim)\n        print(f\"{GREEN}FullRankCoordinator initialized: hidden_dim={hidden_dim}{END}\")\n\n    def reset(self):\n        \"\"\"Reset coordinator state.\"\"\"\n        self.state = np.zeros(self.hidden_dim)\n\n    def step(self, agent_outputs: List[str]) -> Tuple[np.ndarray, str]:\n        \"\"\"Recurrent step: process agent outputs and update state.\"\"\"\n        features = self._encode_outputs(agent_outputs)\n        self.state = self.W @ self.state + features\n        coordinator_message = self._generate_message(self.state)\n        return self.state.copy(), coordinator_message\n\n    def _encode_outputs(self, outputs: List[str]) -> np.ndarray:\n        \"\"\"Encode agent outputs into feature vector.\"\"\"\n        features = []\n        for output in outputs:\n            features.extend([\n                len(output.split()),\n                len(output),\n                output.count('.'),\n                output.count('?'),\n            ])\n\n        features_array = np.array(features[:self.hidden_dim])\n        if len(features_array) < self.hidden_dim:\n            padded = np.zeros(self.hidden_dim)\n            padded[:len(features_array)] = features_array\n            features_array = padded\n\n        features_array = features_array / (np.linalg.norm(features_array) + 1e-8)\n        return features_array\n\n    def _generate_message(self, state: np.ndarray) -> str:\n        \"\"\"Generate coordinator message from state.\"\"\"\n        message_parts = []\n        for i in range(0, len(state), 10):\n            val = state[i]\n            if abs(val) > 0.1:\n                message_parts.append(f\"dim{i}:{val:.2f}\")\n        return \" \".join(message_parts)\n\n\nclass LowRankRecurrentCoordinator:\n    \"\"\"Low-rank recurrent coordinator with rank parameter for ablation.\"\"\"\n\n    def __init__(self, hidden_dim: int = 256, rank: int = 32, num_modules: int = 4):\n        \"\"\"Initialize low-rank coordinator with specified rank.\"\"\"\n        self.hidden_dim = hidden_dim\n        self.rank = rank\n        self.num_modules = num_modules\n\n        # Low-rank factorization: W = U @ V^T\n        self.U = np.random.randn(hidden_dim, rank) * 0.01\n        self.V = np.random.randn(hidden_dim, rank) * 0.01\n\n        # RIM sparse attention\n        self.active_k = max(1, num_modules // 2)\n\n        # Module-specific transformations\n        self.module_weights = [\n            np.random.randn(hidden_dim, rank) * 0.01\n            for _ in range(num_modules)\n        ]\n\n        self.state = np.zeros(hidden_dim)\n\n        compression_ratio = rank / hidden_dim\n        param_reduction = (2 * hidden_dim * rank) / (hidden_dim * hidden_dim)\n\n        print(f\"{GREEN}LowRankCoordinator initialized: rank={rank}, compression={compression_ratio:.2%}{END}\")\n\n    def reset(self):\n        \"\"\"Reset coordinator state.\"\"\"\n        self.state = np.zeros(self.hidden_dim)\n\n    def step(self, agent_outputs: List[str]) -> Tuple[np.ndarray, str]:\n        \"\"\"Low-rank recurrent step with sparse module updates.\"\"\"\n        features = self._encode_outputs(agent_outputs)\n        active_modules = self._select_active_modules(self.state, features)\n\n        # Low-rank update: s_{t+1} = U @ (V^T @ s_t) + module_updates\n        state_proj = self.V.T @ self.state\n        new_state = self.U @ state_proj\n\n        # Apply sparse module updates\n        for module_idx in active_modules:\n            module_update = self.module_weights[module_idx] @ state_proj\n            new_state += module_update\n\n        new_state += features\n        self.state = new_state\n\n        # Compressed message based on rank\n        coordinator_message = self._generate_compressed_message(state_proj)\n        return self.state.copy(), coordinator_message\n\n    def _encode_outputs(self, outputs: List[str]) -> np.ndarray:\n        \"\"\"Encode agent outputs into feature vector.\"\"\"\n        features = []\n        for output in outputs:\n            features.extend([\n                len(output.split()),\n                len(output),\n                output.count('.'),\n                output.count('?'),\n            ])\n\n        features_array = np.array(features[:self.hidden_dim])\n        if len(features_array) < self.hidden_dim:\n            padded = np.zeros(self.hidden_dim)\n            padded[:len(features_array)] = features_array\n            features_array = padded\n\n        features_array = features_array / (np.linalg.norm(features_array) + 1e-8)\n        return features_array\n\n    def _select_active_modules(self, state: np.ndarray, features: np.ndarray) -> List[int]:\n        \"\"\"Select top-k modules based on attention scores.\"\"\"\n        scores = []\n        state_proj = self.V.T @ state\n\n        for module_idx, module_w in enumerate(self.module_weights):\n            module_proj = module_w.T @ features\n            score = np.dot(module_proj[:self.rank], state_proj)\n            scores.append((score, module_idx))\n\n        scores.sort(reverse=True)\n        active = [idx for _, idx in scores[:self.active_k]]\n        return active\n\n    def _generate_compressed_message(self, state_proj: np.ndarray) -> str:\n        \"\"\"Generate compressed coordinator message from projected state.\"\"\"\n        message_parts = []\n        for i in range(len(state_proj)):\n            val = state_proj[i]\n            if abs(val) > 0.1:\n                message_parts.append(f\"r{i}:{val:.2f}\")\n        return \" \".join(message_parts)\n\nprint(\"üß† Coordinator classes implemented successfully!\")\nprint(\"   - FullRankCoordinator: Baseline with full recurrent matrix\")\nprint(\"   - LowRankRecurrentCoordinator: Low-rank version with configurable rank\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üß† Coordinator Implementations\n\nNow let's implement both the baseline full-rank coordinator and the low-rank recurrent coordinator that we'll be ablating.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Color codes for logging\nBLUE, GREEN, YELLOW, CYAN, RED, END = \"\\\\033[94m\", \"\\\\033[92m\", \"\\\\033[93m\", \"\\\\033[96m\", \"\\\\033[91m\", \"\\\\033[0m\"\n\ndef truncate_str(text: str, max_len: int = 100) -> str:\n    \"\"\"Truncate long strings for logging.\"\"\"\n    if len(text) <= max_len:\n        return text\n    return text[:max_len] + f\"... ({len(text)} chars total)\"\n\n@dataclass\nclass ExampleResult:\n    \"\"\"Single example result.\"\"\"\n    input: str\n    output: str\n    context: Dict[str, Any]\n    dataset: str\n    split: str\n    predict_baseline: str\n    predict_method: str\n    method: str\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"Schema matching exp_gen_sol_out.json format.\"\"\"\n    examples: List[Dict[str, Any]]\n\nclass TokenTracker:\n    \"\"\"Track token usage for multi-agent coordination.\"\"\"\n\n    def __init__(self, model: str = \"gpt-4\"):\n        \"\"\"Initialize token tracker with tiktoken encoder.\"\"\"\n        try:\n            self.encoding = tiktoken.encoding_for_model(model)\n            print(f\"{GREEN}TokenTracker initialized with model: {model}{END}\")\n        except Exception as e:\n            print(f\"{YELLOW}Could not load model-specific encoding, using cl100k_base: {e}{END}\")\n            self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n\n        self.total_tokens = 0\n        self.episode_tokens = []\n        self.call_count = 0\n\n    def count_tokens(self, text: str) -> int:\n        \"\"\"Count tokens in text.\"\"\"\n        try:\n            tokens = len(self.encoding.encode(text))\n            return tokens\n        except Exception:\n            # Fallback: approximate as words * 1.3\n            return int(len(text.split()) * 1.3)\n\n    def log_coordinator_step(self, agent_outputs: List[str], coordinator_message: str = \"\"):\n        \"\"\"Log tokens for a coordinator step.\"\"\"\n        step_tokens = 0\n\n        # Count tokens in agent outputs\n        for output in agent_outputs:\n            step_tokens += self.count_tokens(output)\n\n        # Count tokens in coordinator message\n        if coordinator_message:\n            step_tokens += self.count_tokens(coordinator_message)\n\n        self.total_tokens += step_tokens\n        self.episode_tokens.append(step_tokens)\n        self.call_count += 1\n\n        return step_tokens\n\n    def get_stats(self) -> Dict[str, float]:\n        \"\"\"Get aggregated token statistics.\"\"\"\n        return {\n            \"total_tokens\": self.total_tokens,\n            \"num_episodes\": len(self.episode_tokens),\n            \"mean_tokens_per_episode\": np.mean(self.episode_tokens) if self.episode_tokens else 0,\n            \"std_tokens_per_episode\": np.std(self.episode_tokens) if self.episode_tokens else 0,\n            \"call_count\": self.call_count\n        }\n\nprint(\"‚úÖ Utility classes and functions defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üîß Utility Classes and Functions\n\nLet's define the core utility classes for token tracking and helper functions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Embedded dataset - complete multi-LLM coordination examples\nEMBEDDED_DATASET = {\n    \"examples\": [\n        {\n            \"input\": \"What is the difference between OpenCL and CUDA?\",\n            \"output\": \"Winner: model_b\",\n            \"context\": {\n                \"model_a\": \"chatglm-6b\",\n                \"model_b\": \"koala-13b\",\n                \"winner\": \"model_b\",\n                \"judge\": \"arena_user_973\",\n                \"language\": \"English\",\n                \"turn\": 1,\n                \"response_a\": \"OpenCL and CUDA are two different programming models that are used for parallel computing.OpenCL is a general-purposeÂπ∂Ë°åÁºñÁ®ãÊé•Âè£ that allows developers to write parallel code that can run on any platform that supportsCL, which includes most modern operating systems and computer systems, including Windows, Linux, and macOS. It provides a lower-level, more flexible API that is more suitable for building large-scale distributed computing systems.CUDA is a specific implementation ofOpenCL that is designed for performance and scalability in devices with multiple GPU(s). It was developed by Nvidia and is widely used for scientific computing, machine learning, andÂõæÂΩ¢ÂõæÂÉèÂ§ÑÁêÜ. The CUDA API provides a high-level, more efficient way to write parallel code that can run on top of a GPU, while still maintaining the ability to write code that can be easily adapted for other parallel computing platforms.\",\n                \"response_b\": \"OpenCL and CUDA are both programming languages for parallel computing on GPUs, but they differ in several key ways:\\\\n\\\\n1.   Architecture: OpenCL is a general-purpose parallel computing language, while CUDA is designed specifically for GPU computing. OpenCL can run on various types of processors, including CPUs, GPUs, and FPGAs, whereas CUDA is only compatible with NVIDIA GPUs.\\\\n2.   Language: OpenCL is a C++ language, while CUDA is a C++ language with additional features for GPU computing. CUDA provides additional libraries and tools for handling graphics and other tasks unique to GPUs.\\\\n3.   Driver support: OpenCL has a wider range of device support, with support for not just NVIDIA GPUs but also other manufacturers like AMD, Intel, and ARM. CUDA is only available on NVIDIA GPUs, and requires the CUDA Toolkit to be installed on the host machine.\\\\n4.   Memory management: OpenCL provides a unified memory model, where all the data is stored in global memory, and the kernel can access any part of it. CUDA, on the other hand, uses a hierarchical memory architecture, where data is divided into host, device, and shared memory.\\\\n5.   Performance: OpenCL and CUDA have different strengths and weaknesses in terms of performance. OpenCL is generally faster for numerical computations, while CUDA is faster for graphics and other tasks that require fast memory access. CUDA also has better support for 64-bit floating-point operations, which are essential for some scientific applications.\\\\n\\\\nIn summary, OpenCL and CUDA are both programming languages for parallel computing on GPUs, but they differ in terms of architecture, language, driver support, memory management, and performance. OpenCL is a general-purpose language that can run on various types of processors, while CUDA is specific to NVIDIA GPUs and provides additional libraries and tools for handling graphics and other tasks unique to GPUs.\",\n                \"token_usage\": {\n                    \"total_input_tokens\": 181,\n                    \"total_output_tokens_a\": 171,\n                    \"total_output_tokens_b\": 373,\n                    \"total_tokens\": 725\n                }\n            },\n            \"dataset\": \"lmsys/chatbot_arena_conversations\",\n            \"split\": \"train\"\n        },\n        {\n            \"input\": \"Why did my parent not invite me to their wedding?\",\n            \"output\": \"Winner: tie\", \n            \"context\": {\n                \"model_a\": \"oasst-pythia-12b\",\n                \"model_b\": \"alpaca-13b\",\n                \"winner\": \"tie\",\n                \"judge\": \"arena_user_973\",\n                \"language\": \"English\",\n                \"turn\": 1,\n                \"response_a\": \"It is possible that your parent did not invite you to their wedding for a variety of reasons. It could be that they did not want to burden you with the planning or expense of the event, or they may not have had the time or resources to include you. Additionally, it could be that they did not feel comfortable having you attend due to your age or relationship with them. It is important to remember that your parent may have had their own reasons for not inviting you, and it is not your place to judge their choices. If you feel neglected or upset about not being invited, it is best to talk to them about it and express your feelings in a non-accusatory way.\",\n                \"response_b\": \"It is likely that they wanted to keep the guest list small and intimate. Maybe they felt that inviting you would make the event too crowded, or that they wanted to keep the event reserved for close friends and family. It could be that they wanted to avoid any potential drama or conflict that could arise from having too many people at the event. Whatever the reason, it's important to remember that they made this decision out of love and respect for you, and that their decision doesn't define your relationship.\",\n                \"token_usage\": {\n                    \"total_input_tokens\": 150,\n                    \"total_output_tokens_a\": 139,\n                    \"total_output_tokens_b\": 100,\n                    \"total_tokens\": 389\n                }\n            },\n            \"dataset\": \"lmsys/chatbot_arena_conversations\",\n            \"split\": \"train\"\n        },\n        {\n            \"input\": \"Fuji vs. Nikon, which is better?\",\n            \"output\": \"Winner: model_b\",\n            \"context\": {\n                \"model_a\": \"koala-13b\",\n                \"model_b\": \"oasst-pythia-12b\", \n                \"winner\": \"model_b\",\n                \"judge\": \"arena_user_973\",\n                \"language\": \"English\",\n                \"turn\": 1,\n                \"response_a\": \"Both Fuji and Nikon are popular camera brands that offer a wide range of cameras for different types of photography. It really depends on your specific needs and preferences as to which brand is \\\"better\\\" for you.\\\\n\\\\nFujifilm is known for its high image quality, especially in the areas of color accuracy and dynamic range. Many photographers also appreciate the company's focus on ergonomics and the overall user experience of its cameras. Additionally, Fujifilm has a loyal following among enthusiasts and professional photographers, thanks to its commitment to film photography and its high-end mirrorless cameras.\\\\n\\\\nNikon, on the other hand, is known for its advanced autofocus system and image stabilization technology, as well as its high-end cameras and lenses. Nikon also has a wide selection of cameras and lenses to choose from, including both DSLR and mirrorless options.\\\\n\\\\nUltimately, it's difficult to say which brand is \\\"better\\\" without knowing more about your specific needs and preferences. It might be helpful to research both brands further and try out a few different models before making a decision.\",\n                \"response_b\": \"This is a subjective question and the answer depends on the individual preferences and needs. Both Fujifilm and Nikon are well-known for their high-quality cameras, and the choice between the two ultimately depends on the user's personal preferences and needs.\\\\n\\\\nSome factors to consider when deciding between a Fuji and a Nikon camera include:\\\\n\\\\n1. Image quality: Fuji cameras are known for their high-quality images, with a distinctive film-like quality. Nikon cameras are also notable for their image quality, but they may not offer the same level of sharpness and color accuracy as Fuji cameras.\\\\n\\\\n2. Focusing: Fuji cameras are known for their fast and precise autofocus, while Nikon cameras tend to have a slower and more manual focus system.\\\\n\\\\n3. Image size: Fuji cameras are known for their wide range of photo sizes, from small compacts to large-format cameras. Nikon cameras are also capable of producing high-quality images, but they may not be as versatile as Fuji cameras.\\\\n\\\\n4. User interface: Fuji cameras have a user-friendly interface, with a simplified design that is easy to navigate. Nikon cameras may have a more complex interface, but they also offer a wealth of features and customization options.\\\\n\\\\nUltimately, the best camera for you will depend on your specific needs and preferences. If you're looking for a high-quality camera that offers a unique film-like experience, a Fuji camera may be the right choice for you. If you're looking for a camera that is easy to use and offers a wide range of photo sizes, a Nikon camera may be the best choice.\",\n                \"token_usage\": {\n                    \"total_input_tokens\": 222,\n                    \"total_output_tokens_a\": 212,\n                    \"total_output_tokens_b\": 309,\n                    \"total_tokens\": 743\n                }\n            },\n            \"dataset\": \"lmsys/chatbot_arena_conversations\",\n            \"split\": \"train\"\n        },\n        {\n            \"input\": \"How to build an arena for chatbots?\",\n            \"output\": \"Winner: model_b\",\n            \"context\": {\n                \"model_a\": \"vicuna-13b\",\n                \"model_b\": \"oasst-pythia-12b\",\n                \"winner\": \"model_b\",\n                \"judge\": \"arena_user_973\",\n                \"language\": \"English\",\n                \"turn\": 1,\n                \"response_a\": \"Building an arena for chatbots can be done by following these steps:\\\\n\\\\n1. Define the purpose of the arena: The first step is to clearly define the purpose of the arena, such as testing chatbots for a specific task or evaluating their performance in a certain scenario.\\\\n2. Choose a platform: There are several platforms available for building chatbot arenas, such as Dialogflow, Botpress, and Rasa. Choose a platform that best fits your needs and has the necessary features for building the arena.\\\\n3. Create the bot: Create the chatbot that will participate in the arena. This bot should be designed to perform a specific task or set of tasks that are relevant to the purpose of the arena.\\\\n4. Define the evaluation criteria: Determine how the chatbots will be evaluated in the arena. This could include factors such as accuracy, speed, and user experience.\\\\n5. Develop the arena: Use the platform to build the arena, including defining the tasks and scenarios that the chatbots will encounter.\\\\n6. Test and refine: Test the arena with the chatbots to ensure that it is functioning as intended. Refine the arena as needed based on the results of the testing.\\\\n7. Evaluate the chatbots: Use the evaluation criteria to assess the performance of the chatbots in the arena and compare their results.\\\\n\\\\nBy following these steps, you can build an arena for chatbots that will allow you to evaluate and compare their performance on a specific task or set of tasks. This can be useful for improving the accuracy and reliability of chatbots and for identifying areas for further development.\",\n                \"response_b\": \"Building an arena for chatbots is a great way to encourage users to interact with your bot and provides a space for your bot to showcase its abilities. Here are some steps you can follow to build an arena for chatbots:\\\\n\\\\n1. Define the objective: Start by defining the objective of your arena. This could be to create a platform where users can interact with chatbots, ask questions, or simply have conversations.\\\\n\\\\n2. Choose a location: Decide on a location for your arena and make sure it is accessible and has enough space to accommodate the number of bots you plan to have.\\\\n\\\\n3. Set up the infrastructure: Set up the infrastructure needed to run your arena, including a server, database, and programming tools.\\\\n\\\\n4. Design the bots: Design the bots that will be participating in your arena. Consider the various features and capabilities that your bots will need to perform and create a design that will allow them to interact with users in a natural and intuitive way.\\\\n\\\\n5. Develop the software: Develop the software that will run your arena, including the chatbot platform and any other functionality that your bots will need.\\\\n\\\\n6. Test and refine: Test your arena thoroughly and refine it as needed to ensure that it runs smoothly and effectively.\\\\n\\\\n7. Launch and promote: Launch your arena and promote it to your target audience. Encourage users to interact with your bots and provide feedback to help improve the experience.\\\\n\\\\nBy following these steps, you should be able to build an arena for chatbots that provides a platform for users to interact with your bot and provides a space for your bot to showcase its abilities.\",\n                \"token_usage\": {\n                    \"total_input_tokens\": 327,\n                    \"total_output_tokens_a\": 318,\n                    \"total_output_tokens_b\": 320,\n                    \"total_tokens\": 965\n                }\n            },\n            \"dataset\": \"lmsys/chatbot_arena_conversations\",\n            \"split\": \"train\"\n        },\n        {\n            \"input\": \"When is it today?\",\n            \"output\": \"Winner: model_a\",\n            \"context\": {\n                \"model_a\": \"vicuna-13b\",\n                \"model_b\": \"koala-13b\",\n                \"winner\": \"model_a\",\n                \"judge\": \"arena_user_973\",\n                \"language\": \"English\",\n                \"turn\": 1,\n                \"response_a\": \"I'm sorry, I cannot determine the current date for you as I do not have access to the current date and time. My knowledge cutoff is September 2021, and I do not have real-time access to the internet. Please let me know if there is anything else I can assist you with.\",\n                \"response_b\": \"Today is February 23, 2023.\",\n                \"token_usage\": {\n                    \"total_input_tokens\": 66,\n                    \"total_output_tokens_a\": 61,\n                    \"total_output_tokens_b\": 10,\n                    \"total_tokens\": 137\n                }\n            },\n            \"dataset\": \"lmsys/chatbot_arena_conversations\",\n            \"split\": \"train\"\n        }\n    ]\n}\n\nprint(f\"üìä Loaded {len(EMBEDDED_DATASET['examples'])} examples from embedded dataset\")\nprint(\"‚úÖ Dataset is completely self-contained - no external files needed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üìä Embedded Dataset (Self-Contained)\n\nThe dataset below contains multi-LLM coordination examples with token usage annotations. This is a subset of the original dataset embedded directly in the notebook to make it completely self-contained.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import required libraries\nimport json\nimport logging\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Any\nfrom dataclasses import dataclass, asdict\n\nimport numpy as np\nimport tiktoken\nfrom scipy.stats import ttest_rel\nfrom sklearn.metrics import accuracy_score, f1_score\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.use('Agg')  # Non-interactive backend\n\nprint(\"‚úÖ All required libraries imported successfully!\")\nprint(\"üìä Setting up experiment configuration...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Rank-Ablation Study of Low-Rank Recurrent Coordinator on Multi-LLM Coordination Datasets\n\n**Interactive Demo Notebook**\n\n## Experiment Overview\n\n**Hypothesis:** Systematically evaluate how the dimensionality (rank) of the shared recurrent coordinator affects (1) token-efficiency and (2) task performance across multi-LLM interaction datasets.\n\n**Expected Outcome:** If the low-rank recurrent coordinator retains performance while reducing token usage, we will obtain a clear trade-off curve (rank vs. accuracy vs. tokens) that validates the hypothesis.\n\nThis notebook contains a complete, self-contained implementation of the rank ablation experiment that can be run independently without external file dependencies.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}