{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Running the Evaluation\n\nNow let's run the complete evaluation pipeline using our synthetic data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def analyze_rank_ablation(rank_results: Dict) -> Dict:\n    \"\"\"Analyze rank ablation study to find optimal configurations.\"\"\"\n    logger.info(\"Analyzing rank ablation study\")\n\n    try:\n        ranks = sorted([int(r) for r in rank_results.keys()])\n        logger.info(f\"Ranks tested: {ranks}\")\n\n        analysis = {\n            \"rank_comparison\": {},\n            \"optimal_rank\": None,\n            \"diminishing_returns\": []\n        }\n\n        # Compare each rank\n        for rank in ranks:\n            rank_str = str(rank)\n            data = rank_results[rank_str]\n\n            token_mean = data[\"token_stats\"][\"mean_tokens_per_episode\"]\n            accuracy = data[\"metrics\"][\"accuracy\"]\n            efficiency = accuracy / token_mean if token_mean > 0 else 0.0\n\n            analysis[\"rank_comparison\"][rank] = {\n                \"tokens_per_episode\": float(token_mean),\n                \"accuracy\": float(accuracy),\n                \"efficiency_score\": efficiency,\n                \"compression_ratio\": float(data.get(\"compression_ratio\", 0)),\n                \"param_reduction\": float(data.get(\"param_reduction\", 0))\n            }\n\n        # Find optimal rank (best efficiency)\n        best_rank = max(\n            analysis[\"rank_comparison\"].items(),\n            key=lambda x: x[1][\"efficiency_score\"]\n        )[0]\n\n        logger.info(f\"Optimal rank identified: {best_rank}\")\n\n        analysis[\"optimal_rank\"] = {\n            \"rank\": int(best_rank),\n            \"rationale\": f\"Rank {best_rank} achieves best efficiency score (accuracy/tokens)\"\n        }\n\n        # Check for diminishing returns\n        for i in range(len(ranks) - 1):\n            r1, r2 = ranks[i], ranks[i+1]\n            eff1 = analysis[\"rank_comparison\"][r1][\"efficiency_score\"]\n            eff2 = analysis[\"rank_comparison\"][r2][\"efficiency_score\"]\n\n            improvement = ((eff2 - eff1) / eff1) * 100 if eff1 > 0 else 0.0\n\n            if improvement < 1.0:  # Less than 1% improvement\n                analysis[\"diminishing_returns\"].append({\n                    \"from_rank\": int(r1),\n                    \"to_rank\": int(r2),\n                    \"efficiency_improvement_percent\": improvement,\n                    \"note\": \"Diminishing returns observed\"\n                })\n\n        logger.info(f\"Diminishing returns found: {len(analysis['diminishing_returns'])} cases\")\n\n        return analysis\n\n    except Exception as e:\n        logger.error(f\"Error analyzing rank ablation: {e}\")\n        raise\n\nprint(\"✓ Rank ablation analysis function defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Rank Ablation Analysis\n\nThis function analyzes different rank configurations to find the optimal setting.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_token_efficiency_metrics(\n    baseline_tokens: List[int],\n    method_tokens: List[int],\n    baseline_acc: float,\n    method_acc: float\n) -> Dict:\n    \"\"\"Compute comprehensive token efficiency metrics.\"\"\"\n    logger.info(\"Computing token efficiency metrics\")\n\n    try:\n        # Validate inputs\n        if len(baseline_tokens) != len(method_tokens):\n            raise ValueError(f\"Token arrays have different lengths: {len(baseline_tokens)} vs {len(method_tokens)}\")\n\n        if len(baseline_tokens) == 0:\n            raise ValueError(\"Empty token arrays\")\n\n        logger.info(f\"Number of samples: {len(baseline_tokens)}\")\n        logger.info(f\"Baseline tokens sample: {truncate_for_log(baseline_tokens[:5])}\")\n        logger.info(f\"Method tokens sample: {truncate_for_log(method_tokens[:5])}\")\n\n        # Token statistics\n        baseline_mean = float(np.mean(baseline_tokens))\n        baseline_std = float(np.std(baseline_tokens))\n        method_mean = float(np.mean(method_tokens))\n        method_std = float(np.std(method_tokens))\n\n        logger.info(f\"Baseline: mean={baseline_mean:.2f}, std={baseline_std:.2f}\")\n        logger.info(f\"Method: mean={method_mean:.2f}, std={method_std:.2f}\")\n\n        # Token reduction\n        token_reduction = ((baseline_mean - method_mean) / baseline_mean) * 100 if baseline_mean > 0 else 0.0\n        absolute_reduction = baseline_mean - method_mean\n\n        logger.info(f\"Token reduction: {token_reduction:.2f}% ({absolute_reduction:.2f} tokens)\")\n\n        # Efficiency score: accuracy / tokens (higher is better)\n        baseline_efficiency = baseline_acc / baseline_mean if baseline_mean > 0 else 0.0\n        method_efficiency = method_acc / method_mean if method_mean > 0 else 0.0\n        efficiency_improvement = ((method_efficiency - baseline_efficiency) / baseline_efficiency) * 100 if baseline_efficiency > 0 else 0.0\n\n        logger.info(f\"Efficiency improvement: {efficiency_improvement:.2f}%\")\n\n        # Statistical significance test (paired t-test)\n        t_stat, p_value = stats.ttest_rel(baseline_tokens, method_tokens)\n\n        logger.info(f\"Statistical test: t={t_stat:.4f}, p={p_value:.2e}\")\n\n        # Effect size (Cohen's d for paired samples)\n        diff = np.array(baseline_tokens) - np.array(method_tokens)\n        cohens_d = float(np.mean(diff) / np.std(diff)) if np.std(diff) > 0 else 0.0\n\n        # Task success rate comparison\n        accuracy_delta = method_acc - baseline_acc\n        accuracy_maintained = abs(accuracy_delta) < 0.01  # Within 1% tolerance\n\n        logger.info(f\"Accuracy delta: {accuracy_delta:.4f}, maintained: {accuracy_maintained}\")\n\n        return {\n            \"baseline_stats\": {\n                \"mean_tokens\": baseline_mean,\n                \"std_tokens\": baseline_std,\n                \"accuracy\": float(baseline_acc),\n                \"efficiency_score\": baseline_efficiency\n            },\n            \"method_stats\": {\n                \"mean_tokens\": method_mean,\n                \"std_tokens\": method_std,\n                \"accuracy\": float(method_acc),\n                \"efficiency_score\": method_efficiency\n            },\n            \"improvements\": {\n                \"token_reduction_percent\": token_reduction,\n                \"token_reduction_absolute\": absolute_reduction,\n                \"efficiency_improvement_percent\": efficiency_improvement,\n                \"accuracy_delta\": accuracy_delta,\n                \"accuracy_maintained\": accuracy_maintained\n            },\n            \"statistical_tests\": {\n                \"t_statistic\": float(t_stat),\n                \"p_value\": float(p_value),\n                \"significant_at_0.05\": bool(p_value < 0.05),\n                \"significant_at_0.01\": bool(p_value < 0.01),\n                \"cohens_d\": cohens_d,\n                \"effect_size_interpretation\": interpret_effect_size(cohens_d)\n            }\n        }\n\n    except Exception as e:\n        logger.error(f\"Error computing efficiency metrics: {e}\")\n        raise\n\nprint(\"✓ Token efficiency metrics function defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Token Efficiency Metrics Calculation\n\nThe core function computes comprehensive token efficiency metrics including statistical tests and effect sizes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def truncate_for_log(data: Any, max_len: int = 200) -> str:\n    \"\"\"Truncate data for logging to avoid long output.\"\"\"\n    s = str(data)\n    if len(s) > max_len:\n        return s[:max_len] + f\"... (truncated, total length: {len(s)})\"\n    return s\n\ndef interpret_effect_size(d: float) -> str:\n    \"\"\"Interpret Cohen's d effect size.\"\"\"\n    abs_d = abs(d)\n    if abs_d < 0.2:\n        return \"negligible\"\n    elif abs_d < 0.5:\n        return \"small\"\n    elif abs_d < 0.8:\n        return \"medium\"\n    else:\n        return \"large\"\n\nprint(\"✓ Helper functions defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Helper Functions\n\nThe evaluation uses several helper functions to compute token efficiency metrics and analyze results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Synthetic experiment data (based on actual results from the evaluation)\n# This represents the data that would normally be loaded from method_summary.json files\n\n# Experiment 1: Empirical Evaluation\nexp1_data = {\n    \"method_summary\": {\n        \"mean_tokens_per_episode\": 318.5,\n        \"accuracy\": 0.525,\n        \"dataset_size\": 200,\n        \"compression_ratio\": 0.012\n    },\n    \"baseline_results\": {\n        \"mean_tokens_per_episode\": 320.715,\n        \"accuracy\": 0.525\n    }\n}\n\n# Experiment 2: Rank Ablation Study with detailed per-rank results\nexp2_data = {\n    \"dataset_size\": 200,\n    \"baseline_results\": {\n        \"token_counts\": np.random.normal(320.715, 50, 200).astype(int).tolist(),\n        \"predictions\": [\"model_a\" if i % 3 == 0 else \"model_b\" if i % 3 == 1 else \"tie\" for i in range(200)],\n        \"metrics\": {\n            \"accuracy\": 0.525\n        }\n    },\n    \"rank_ablation_results\": {\n        \"2\": {\n            \"token_counts\": np.random.normal(318, 52, 200).astype(int).tolist(),\n            \"predictions\": [\"model_a\" if i % 3 == 0 else \"model_b\" if i % 3 == 1 else \"tie\" for i in range(200)],\n            \"metrics\": {\"accuracy\": 0.52},\n            \"token_stats\": {\"mean_tokens_per_episode\": 318.0},\n            \"compression_ratio\": 0.008,\n            \"param_reduction\": 0.75\n        },\n        \"4\": {\n            \"token_counts\": np.random.normal(317.2, 51, 200).astype(int).tolist(),\n            \"predictions\": [\"model_a\" if i % 3 == 0 else \"model_b\" if i % 3 == 1 else \"tie\" for i in range(200)],\n            \"metrics\": {\"accuracy\": 0.523},\n            \"token_stats\": {\"mean_tokens_per_episode\": 317.2},\n            \"compression_ratio\": 0.011,\n            \"param_reduction\": 0.50\n        },\n        \"8\": {\n            \"token_counts\": np.random.normal(316.785, 49, 200).astype(int).tolist(),\n            \"predictions\": [\"model_a\" if i % 3 == 0 else \"model_b\" if i % 3 == 1 else \"tie\" for i in range(200)],\n            \"metrics\": {\"accuracy\": 0.525},\n            \"token_stats\": {\"mean_tokens_per_episode\": 316.785},\n            \"compression_ratio\": 0.012,\n            \"param_reduction\": 0.25\n        },\n        \"16\": {\n            \"token_counts\": np.random.normal(316.9, 48.5, 200).astype(int).tolist(),\n            \"predictions\": [\"model_a\" if i % 3 == 0 else \"model_b\" if i % 3 == 1 else \"tie\" for i in range(200)],\n            \"metrics\": {\"accuracy\": 0.524},\n            \"token_stats\": {\"mean_tokens_per_episode\": 316.9},\n            \"compression_ratio\": 0.012,\n            \"param_reduction\": 0.125\n        }\n    }\n}\n\nprint(\"✓ Synthetic experimental data loaded successfully\")\nprint(f\"Dataset size: {exp2_data['dataset_size']}\")\nprint(f\"Ranks tested: {list(exp2_data['rank_ablation_results'].keys())}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Data\n\nInstead of loading from external JSON files, we'll create synthetic data that represents the experimental results from two studies:\n1. **Experiment 1**: Empirical evaluation of Low-Rank Recurrent Coordinator\n2. **Experiment 2**: Rank ablation study with multiple rank configurations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import required libraries\nimport json\nimport logging\nimport numpy as np\nfrom scipy import stats\nfrom typing import Dict, List, Any\n\n# Configure logging for demo\nlogging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Cross-Task Token Efficiency Evaluation of Low-Rank Coordinator\n\nThis notebook demonstrates the evaluation methodology for measuring token efficiency improvements achieved by the Low-Rank Recurrent Coordinator compared to baseline methods.\n\n## Overview\n\nThe evaluation analyzes:\n- **Token Usage**: Comparing token consumption between baseline and low-rank coordinator methods\n- **Accuracy Preservation**: Ensuring task performance is maintained while reducing tokens\n- **Statistical Significance**: Using paired t-tests to validate improvements\n- **Rank Optimization**: Finding optimal low-rank configurations through ablation studies\n\nThe original script evaluated 200 coordination tasks from LMSYS chatbot arena conversations, achieving 1.23% token reduction while maintaining 52.5% accuracy.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}