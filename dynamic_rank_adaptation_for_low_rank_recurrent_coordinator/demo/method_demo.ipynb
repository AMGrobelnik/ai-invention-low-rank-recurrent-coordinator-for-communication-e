{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Sample Dataset\n\nWe'll use a small sample dataset of multi-agent conversations for demonstration. In the original experiment, this data would be loaded from external JSON files, but here we've inlined it to make the notebook self-contained.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def predict_winner(state: np.ndarray, agent_outputs: List[str], context: Dict) -> str:\n    \"\"\"Predict winner based on coordinator state.\"\"\"\n    try:\n        len_a = len(agent_outputs[0])\n        len_b = len(agent_outputs[1])\n        \n        confidence = np.sum(state[state > 0])\n        \n        score_a = len_a * (1 + confidence * 0.1)\n        score_b = len_b * (1 + confidence * 0.1)\n        \n        if abs(score_a - score_b) < 50:\n            return \"tie\"\n        elif score_a > score_b:\n            return \"model_a\"\n        else:\n            return \"model_b\"\n    except Exception as e:\n        logger.error(f\"Error in predict_winner: {e}\")\n        return \"tie\"\n\ndef run_experiment(examples: List[Dict], coordinator, tracker: TokenTracker, name: str) -> Tuple[List[str], List[int]]:\n    \"\"\"Run experiment with given coordinator.\"\"\"\n    logger.info(f\"Running experiment: {name}\")\n    \n    predictions = []\n    token_counts = []\n    \n    for idx, example in enumerate(examples):\n        if (idx + 1) % 5 == 0 or len(examples) <= 10:\n            print(f\"Processing example {idx+1}/{len(examples)}\")\n            \n        # Reset coordinator state for new episode\n        coordinator.reset()\n        \n        # Track tokens for this episode\n        episode_start_tokens = tracker.total_tokens\n        \n        # Extract agent outputs from dataset\n        context = example['context']\n        agent_outputs = [\n            context['response_a'],\n            context['response_b']\n        ]\n        \n        # Coordinator processes both agent responses\n        state, coordinator_message = coordinator.step(agent_outputs)\n        \n        # Track tokens\n        step_tokens = tracker.log_coordinator_step(agent_outputs, coordinator_message)\n        \n        # Make prediction\n        prediction = predict_winner(state, agent_outputs, context)\n        predictions.append(prediction)\n        \n        # Record tokens for this episode\n        episode_tokens = tracker.total_tokens - episode_start_tokens\n        token_counts.append(episode_tokens)\n    \n    logger.info(f\"Experiment {name} complete: {len(predictions)} predictions made\")\n    return predictions, token_counts\n\ndef evaluate_performance(predictions: List[str], ground_truth: List[str]) -> Dict[str, float]:\n    \"\"\"Evaluate task performance metrics.\"\"\"\n    try:\n        accuracy = accuracy_score(ground_truth, predictions)\n        f1_macro = f1_score(ground_truth, predictions, average='macro', zero_division=0)\n        f1_weighted = f1_score(ground_truth, predictions, average='weighted', zero_division=0)\n        \n        return {\n            \"accuracy\": float(accuracy),\n            \"f1_macro\": float(f1_macro),\n            \"f1_weighted\": float(f1_weighted)\n        }\n    except Exception as e:\n        logger.error(f\"Error evaluating performance: {e}\")\n        return {\n            \"accuracy\": 0.0,\n            \"f1_macro\": 0.0,\n            \"f1_weighted\": 0.0\n        }\n\ndef compute_statistical_significance(baseline_tokens: List[int], method_tokens: List[int]) -> Dict[str, Any]:\n    \"\"\"Test if improvement is statistically significant.\"\"\"\n    try:\n        t_stat, t_pval = ttest_rel(baseline_tokens, method_tokens)\n        t_significant = t_pval < 0.05\n        \n        mean_diff = np.mean(baseline_tokens) - np.mean(method_tokens)\n        pooled_std = np.sqrt((np.var(baseline_tokens) + np.var(method_tokens)) / 2)\n        cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0\n        \n        return {\n            \"t_statistic\": float(t_stat),\n            \"t_pvalue\": float(t_pval),\n            \"t_significant\": bool(t_significant),\n            \"cohens_d\": float(cohens_d),\n            \"alpha\": 0.05\n        }\n    except Exception as e:\n        logger.error(f\"Error computing statistical significance: {e}\")\n        return {\n            \"t_statistic\": 0.0,\n            \"t_pvalue\": 1.0,\n            \"t_significant\": False,\n            \"cohens_d\": 0.0,\n            \"alpha\": 0.05\n        }\n\nprint(\"✅ Experimental functions defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Experimental Functions\n\nThese functions handle running experiments and evaluating performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class DynamicRankCoordinator:\n    \"\"\"\n    NOVEL: Dynamic rank adaptation coordinator.\n\n    Key innovation: Rank adapts based on episode complexity:\n    - Starts with minimal rank (8)\n    - Increases rank if uncertainty is high (low prediction confidence)\n    - Decreases rank if task is simple (high confidence)\n    - Max rank = 64 (still more compressed than full-rank 256)\n    \"\"\"\n\n    def __init__(self, hidden_dim: int = 256, min_rank: int = 8, max_rank: int = 64, num_modules: int = 4):\n        self.hidden_dim = hidden_dim\n        self.min_rank = min_rank\n        self.max_rank = max_rank\n        self.current_rank = min_rank  # Start with minimal rank\n        self.num_modules = num_modules\n\n        # Initialize matrices for max rank (we'll only use first current_rank columns)\n        self.U = np.random.randn(hidden_dim, max_rank) * 0.01\n        self.V = np.random.randn(hidden_dim, max_rank) * 0.01\n\n        # RIM sparse attention\n        self.active_k = max(1, num_modules // 2)\n        self.module_weights = [\n            np.random.randn(hidden_dim, max_rank) * 0.01\n            for _ in range(num_modules)\n        ]\n\n        self.state = np.zeros(hidden_dim)\n        self.rank_history = []  # Track rank adaptation history\n\n        logger.info(f\"DynamicRankCoordinator initialized: min_rank={min_rank}, max_rank={max_rank}, modules={num_modules}\")\n\n    def reset(self):\n        \"\"\"Reset coordinator state.\"\"\"\n        self.state = np.zeros(self.hidden_dim)\n        self.current_rank = self.min_rank  # Reset to minimum rank for new episode\n\n    def step(self, agent_outputs: List[str]) -> Tuple[np.ndarray, str]:\n        \"\"\"Dynamic rank recurrent step.\"\"\"\n        features = self._encode_outputs(agent_outputs)\n\n        # ADAPTIVE RANK SELECTION based on input complexity\n        self._adapt_rank(agent_outputs, features)\n\n        active_modules = self._select_active_modules(self.state, features)\n\n        # Low-rank update using CURRENT rank (not full max_rank)\n        state_proj = self.V[:, :self.current_rank].T @ self.state  # Project to current_rank space\n        new_state = self.U[:, :self.current_rank] @ state_proj  # Reconstruct\n\n        # Apply sparse module updates (only using current rank)\n        for module_idx in active_modules:\n            module_update = self.module_weights[module_idx][:, :self.current_rank] @ state_proj\n            new_state += module_update\n\n        new_state += features\n        self.state = new_state\n\n        # Generate compressed message (size depends on current rank)\n        coordinator_message = self._generate_compressed_message(state_proj)\n\n        # Track rank for analysis\n        self.rank_history.append(self.current_rank)\n\n        return self.state.copy(), coordinator_message\n\n    def _adapt_rank(self, agent_outputs: List[str], features: np.ndarray):\n        \"\"\"\n        Adapt rank based on episode complexity.\n        \n        Complexity signals:\n        1. Length variance between agents (high variance = complex disagreement)\n        2. Feature magnitude (high magnitude = complex input)\n        3. State uncertainty (high variance in state = need more capacity)\n        \"\"\"\n        # Signal 1: Length variance\n        lengths = [len(out) for out in agent_outputs]\n        length_variance = np.var(lengths) if len(lengths) > 1 else 0\n\n        # Signal 2: Feature magnitude\n        feature_magnitude = np.linalg.norm(features)\n\n        # Signal 3: State uncertainty (variance across dimensions)\n        state_uncertainty = np.var(self.state) if np.any(self.state) else 0\n\n        # Combine signals into complexity score (normalized)\n        complexity_score = (\n            (length_variance / 10000) * 0.4 +  # Normalize length variance\n            feature_magnitude * 0.3 +  # Feature magnitude already [0, 1]\n            (state_uncertainty * 10) * 0.3  # State uncertainty\n        )\n\n        # Map complexity to rank\n        if complexity_score < 0.3:\n            target_rank = self.min_rank\n        elif complexity_score < 0.7:\n            # Linear interpolation between min and max\n            progress = (complexity_score - 0.3) / 0.4\n            target_rank = int(self.min_rank + progress * (self.max_rank - self.min_rank))\n        else:\n            target_rank = self.max_rank\n\n        # Smooth adaptation (don't jump too fast)\n        if target_rank > self.current_rank:\n            self.current_rank = min(self.current_rank + 8, target_rank)\n        elif target_rank < self.current_rank:\n            self.current_rank = max(self.current_rank - 4, target_rank)\n\n        # Ensure bounds\n        self.current_rank = max(self.min_rank, min(self.max_rank, self.current_rank))\n\n    def _encode_outputs(self, outputs: List[str]) -> np.ndarray:\n        \"\"\"Encode agent outputs into feature vector.\"\"\"\n        features = []\n        for output in outputs:\n            features.extend([\n                len(output.split()),\n                len(output),\n                output.count('.'),\n                output.count('?'),\n            ])\n\n        features_array = np.array(features[:self.hidden_dim])\n        if len(features_array) < self.hidden_dim:\n            padded = np.zeros(self.hidden_dim)\n            padded[:len(features_array)] = features_array\n            features_array = padded\n\n        features_array = features_array / (np.linalg.norm(features_array) + 1e-8)\n        return features_array\n\n    def _select_active_modules(self, state: np.ndarray, features: np.ndarray) -> List[int]:\n        \"\"\"Select top-k modules based on attention scores.\"\"\"\n        scores = []\n        state_proj = self.V[:, :self.current_rank].T @ state\n\n        for module_idx, module_w in enumerate(self.module_weights):\n            module_proj = module_w[:, :self.current_rank].T @ features\n            score = np.dot(module_proj, state_proj)\n            scores.append((score, module_idx))\n\n        scores.sort(reverse=True)\n        return [idx for _, idx in scores[:self.active_k]]\n\n    def _generate_compressed_message(self, state_proj: np.ndarray) -> str:\n        \"\"\"Generate compressed coordinator message from projected state.\"\"\"\n        message_parts = []\n        for i in range(len(state_proj)):\n            val = state_proj[i]\n            if abs(val) > 0.1:\n                message_parts.append(f\"r{i}:{val:.2f}\")\n        return \" \".join(message_parts)\n\n    def get_rank_stats(self) -> Dict[str, float]:\n        \"\"\"Get rank adaptation statistics.\"\"\"\n        if not self.rank_history:\n            return {}\n\n        return {\n            \"mean_rank\": float(np.mean(self.rank_history)),\n            \"std_rank\": float(np.std(self.rank_history)),\n            \"min_rank_used\": int(np.min(self.rank_history)),\n            \"max_rank_used\": int(np.max(self.rank_history)),\n            \"rank_changes\": int(np.sum(np.abs(np.diff(self.rank_history)) > 0))\n        }\n\nprint(\"✅ DynamicRankCoordinator class defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class StaticLowRankCoordinator:\n    \"\"\"Static low-rank coordinator with fixed rank (baseline method from exp_2_006).\"\"\"\n\n    def __init__(self, hidden_dim: int = 256, rank: int = 32, num_modules: int = 4):\n        self.hidden_dim = hidden_dim\n        self.rank = rank\n        self.num_modules = num_modules\n\n        # Low-rank factorization: W = U @ V^T\n        self.U = np.random.randn(hidden_dim, rank) * 0.01\n        self.V = np.random.randn(hidden_dim, rank) * 0.01\n\n        # RIM sparse attention\n        self.active_k = max(1, num_modules // 2)\n        self.module_weights = [\n            np.random.randn(hidden_dim, rank) * 0.01\n            for _ in range(num_modules)\n        ]\n\n        self.state = np.zeros(hidden_dim)\n        logger.info(f\"StaticLowRankCoordinator initialized: rank={rank}, modules={num_modules}\")\n\n    def reset(self):\n        \"\"\"Reset coordinator state.\"\"\"\n        self.state = np.zeros(self.hidden_dim)\n\n    def step(self, agent_outputs: List[str]) -> Tuple[np.ndarray, str]:\n        \"\"\"Low-rank recurrent step with sparse module updates.\"\"\"\n        features = self._encode_outputs(agent_outputs)\n        active_modules = self._select_active_modules(self.state, features)\n\n        # Low-rank update\n        state_proj = self.V.T @ self.state\n        new_state = self.U @ state_proj\n\n        # Apply sparse module updates\n        for module_idx in active_modules:\n            module_update = self.module_weights[module_idx] @ state_proj\n            new_state += module_update\n\n        new_state += features\n        self.state = new_state\n\n        # Generate compressed message (only rank dimensions)\n        coordinator_message = self._generate_compressed_message(state_proj)\n        return self.state.copy(), coordinator_message\n\n    def _encode_outputs(self, outputs: List[str]) -> np.ndarray:\n        \"\"\"Encode agent outputs into feature vector.\"\"\"\n        features = []\n        for output in outputs:\n            features.extend([\n                len(output.split()),\n                len(output),\n                output.count('.'),\n                output.count('?'),\n            ])\n\n        features_array = np.array(features[:self.hidden_dim])\n        if len(features_array) < self.hidden_dim:\n            padded = np.zeros(self.hidden_dim)\n            padded[:len(features_array)] = features_array\n            features_array = padded\n\n        features_array = features_array / (np.linalg.norm(features_array) + 1e-8)\n        return features_array\n\n    def _select_active_modules(self, state: np.ndarray, features: np.ndarray) -> List[int]:\n        \"\"\"Select top-k modules based on attention scores.\"\"\"\n        scores = []\n        state_proj = self.V.T @ state\n\n        for module_idx, module_w in enumerate(self.module_weights):\n            module_proj = module_w.T @ features\n            score = np.dot(module_proj[:self.rank], state_proj)\n            scores.append((score, module_idx))\n\n        scores.sort(reverse=True)\n        return [idx for _, idx in scores[:self.active_k]]\n\n    def _generate_compressed_message(self, state_proj: np.ndarray) -> str:\n        \"\"\"Generate compressed coordinator message from projected state.\"\"\"\n        message_parts = []\n        for i in range(len(state_proj)):\n            val = state_proj[i]\n            if abs(val) > 0.1:\n                message_parts.append(f\"r{i}:{val:.2f}\")\n        return \" \".join(message_parts)\n\nprint(\"✅ StaticLowRankCoordinator class defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class FullRankCoordinator:\n    \"\"\"Baseline: Full-rank recurrent coordinator.\"\"\"\n\n    def __init__(self, hidden_dim: int = 256):\n        self.hidden_dim = hidden_dim\n        self.W = np.random.randn(hidden_dim, hidden_dim) * 0.01\n        self.state = np.zeros(hidden_dim)\n        logger.info(f\"FullRankCoordinator initialized: hidden_dim={hidden_dim}\")\n\n    def reset(self):\n        \"\"\"Reset coordinator state.\"\"\"\n        self.state = np.zeros(self.hidden_dim)\n\n    def step(self, agent_outputs: List[str]) -> Tuple[np.ndarray, str]:\n        \"\"\"Recurrent step.\"\"\"\n        features = self._encode_outputs(agent_outputs)\n        self.state = self.W @ self.state + features\n        coordinator_message = self._generate_message(self.state)\n        return self.state.copy(), coordinator_message\n\n    def _encode_outputs(self, outputs: List[str]) -> np.ndarray:\n        \"\"\"Encode agent outputs into feature vector.\"\"\"\n        features = []\n        for output in outputs:\n            features.extend([\n                len(output.split()),\n                len(output),\n                output.count('.'),\n                output.count('?'),\n            ])\n\n        features_array = np.array(features[:self.hidden_dim])\n        if len(features_array) < self.hidden_dim:\n            padded = np.zeros(self.hidden_dim)\n            padded[:len(features_array)] = features_array\n            features_array = padded\n\n        features_array = features_array / (np.linalg.norm(features_array) + 1e-8)\n        return features_array\n\n    def _generate_message(self, state: np.ndarray) -> str:\n        \"\"\"Generate coordinator message from state.\"\"\"\n        message_parts = []\n        for i in range(0, len(state), 10):\n            val = state[i]\n            if abs(val) > 0.1:\n                message_parts.append(f\"dim{i}:{val:.2f}\")\n        return \" \".join(message_parts)\n\nprint(\"✅ FullRankCoordinator class defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Coordinator Implementations\n\nNow we'll implement the three different coordinator types:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class TokenTracker:\n    \"\"\"Track token usage for multi-agent coordination.\"\"\"\n\n    def __init__(self, model: str = \"gpt-4\"):\n        \"\"\"Initialize token tracker with tiktoken encoder.\"\"\"\n        try:\n            self.encoding = tiktoken.encoding_for_model(model)\n            logger.info(f\"TokenTracker initialized with model: {model}\")\n        except Exception as e:\n            logger.warning(f\"Could not load model-specific encoding, using cl100k_base: {e}\")\n            self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n\n        self.total_tokens = 0\n        self.episode_tokens = []\n        self.call_count = 0\n\n    def count_tokens(self, text: str) -> int:\n        \"\"\"Count tokens in text.\"\"\"\n        try:\n            tokens = len(self.encoding.encode(text))\n            return tokens\n        except Exception as e:\n            logger.debug(f\"Error counting tokens: {e}\")\n            # Fallback: approximate as words * 1.3\n            return int(len(text.split()) * 1.3)\n\n    def log_coordinator_step(self, agent_outputs: List[str], coordinator_message: str = \"\"):\n        \"\"\"Log tokens for a coordinator step with comprehensive validation.\"\"\"\n        try:\n            if not isinstance(agent_outputs, list):\n                logger.error(f\"agent_outputs must be a list, got {type(agent_outputs)}\")\n                raise TypeError(f\"agent_outputs must be a list, got {type(agent_outputs)}\")\n\n            step_tokens = 0\n\n            # Count tokens in agent outputs\n            for idx, output in enumerate(agent_outputs):\n                if not isinstance(output, str):\n                    logger.warning(f\"Agent output {idx} is not a string: {type(output)}\")\n                    output = str(output)\n\n                output_tokens = self.count_tokens(output)\n                step_tokens += output_tokens\n\n            # Count tokens in coordinator message\n            if coordinator_message:\n                if not isinstance(coordinator_message, str):\n                    coordinator_message = str(coordinator_message)\n\n                coord_tokens = self.count_tokens(coordinator_message)\n                step_tokens += coord_tokens\n\n            # Update tracking\n            self.total_tokens += step_tokens\n            self.episode_tokens.append(step_tokens)\n            self.call_count += 1\n\n            return step_tokens\n\n        except Exception as e:\n            logger.error(f\"Error logging coordinator step: {e}\")\n            return 0\n\n    def get_stats(self) -> Dict[str, float]:\n        \"\"\"Get aggregated token statistics.\"\"\"\n        return {\n            \"total_tokens\": self.total_tokens,\n            \"num_episodes\": len(self.episode_tokens),\n            \"mean_tokens_per_episode\": np.mean(self.episode_tokens) if self.episode_tokens else 0,\n            \"std_tokens_per_episode\": np.std(self.episode_tokens) if self.episode_tokens else 0,\n            \"call_count\": self.call_count\n        }\n\nprint(\"✅ TokenTracker class defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Token Tracking System\n\nThe TokenTracker class monitors token usage across different coordination steps.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass ExampleResult:\n    \"\"\"Single example result.\"\"\"\n    input: str\n    output: str\n    context: Dict[str, Any]\n    dataset: str\n    split: str\n    predict_baseline: str\n    predict_static: str\n    predict_dynamic: str\n    method: str\n\n@dataclass\nclass ExperimentResult:\n    \"\"\"Schema matching exp_gen_sol_out.json format.\"\"\"\n    examples: List[Dict[str, Any]]\n\ndef truncate_str(text: str, max_len: int = 100) -> str:\n    \"\"\"Truncate long strings for logging.\"\"\"\n    if len(text) <= max_len:\n        return text\n    return text[:max_len] + f\"... ({len(text)} chars total)\"\n\nprint(\"✅ Data classes and helper functions defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Classes and Helper Functions\n\nFirst, let's define the data structures and utility functions used throughout the experiment.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import required libraries\nimport json\nimport logging\nimport numpy as np\nimport tiktoken\nfrom scipy.stats import ttest_rel\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom typing import Dict, List, Tuple, Any\nfrom dataclasses import dataclass, asdict\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set up logging for the notebook\nlogging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\nlogger = logging.getLogger(__name__)\n\nprint(\"✅ All libraries imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Dynamic Rank Adaptation for Low-Rank Recurrent Coordinator\n\nThis notebook demonstrates an experiment comparing different coordination mechanisms for multi-LLM systems:\n\n1. **Baseline**: Full-rank recurrent coordinator (256×256 parameters)\n2. **Static Low-Rank**: Fixed rank=32 coordinator \n3. **Dynamic Adaptive Rank**: Rank adapts based on episode complexity (8-64 range)\n\n## Key Innovation\n\nThe dynamic rank adaptation mechanism automatically adjusts the coordinator's representational capacity based on episode complexity:\n- Starts with minimal rank (8) for simple tasks\n- Increases rank when uncertainty is high (low prediction confidence)\n- Decreases rank when task is simple (high confidence)\n- Maximum rank = 64 (still more compressed than full-rank 256)\n\nThis aims to achieve better token efficiency by using low rank for simple episodes and higher rank only when needed.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}