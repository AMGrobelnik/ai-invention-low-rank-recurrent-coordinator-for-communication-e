# Sources

## 1. Recurrent Independent Mechanisms

**URL:** https://arxiv.org/abs/1909.10893

**Summary:** Introduces RIMs: modular recurrent groups that update selectively and communicate sparsely through an attention bottleneck; shows specialization and improved generalization.

---

## 2. TokenSqueeze: Performance‑Preserving Compression for Reasoning LLMs

**URL:** https://openreview.net/pdf?id=Wc1VZ2bVJn

**Summary:** Proposes TokenSqueeze: adaptive reasoning‑depth selection, intra‑step linguistic refinement, and length‑aware preference training to reduce token usage (reports ~50% token reduction on MATH500 while preserving accuracy).

---

## 3. CCF: A Context Compression Framework

**URL:** https://arxiv.org/abs/2509.09199v1

**Summary:** Presents a learned hierarchical context compression framework that aggregates segments and encodes KV memory to form compact representations with competitive perplexity and improved throughput under high compression ratios.

---

