# Sources

## 1. Recurrent Independent Mechanisms (OpenReview)

**URL:** https://openreview.net/forum?id=BylaUTNtPS

**Summary:** Describes RIMs: multiple recurrent modules with sparse updates and communication via attention bottleneck; used to support claims about sparse modular recurrence and reduced communication.

---

## 2. TokenSqueeze: Performance‑Preserving Compression for Reasoning LLMs (arXiv PDF)

**URL:** https://arxiv.org/pdf/2511.13223

**Summary:** Presents TokenSqueeze, a Long→Short training method that compresses reasoning traces to reduce token usage while aiming to preserve accuracy; used to support claims about token‑efficient reasoning compression.

---

## 3. Literature Survey on Low‑Rank Recurrent Coordinators and Token Compression

**URL:** user-provided: Literature Survey on Low-Rank Recurrent Coordinators and Token Compression (fin_1_001)

**Summary:** Surveyed RIMs, learned hierarchical context/latent compression (CCF), and TokenSqueeze; concluded no end‑to‑end coordinator combining all three exists and recommended integrated experiments and ablations.

---

## 4. Proof of Rank‑Bound on Recurrent Coordinator State

**URL:** user-provided: Proof of Rank‑Bound on Recurrent Coordinator State (pro_1_002)

**Summary:** Provides a formal proof establishing a rank bound on recurrent coordinator state, constraining representational capacity for low‑dimensional coordinators.

---

## 5. Multi‑Agent Coordination Communication‑Efficiency Dataset

**URL:** user-provided: Multi-Agent Coordination Communication‑Efficiency Dataset (dat_1_003)

**Summary:** Curated dataset of 200 multi‑LLM interaction episodes with per‑turn prompts, model responses, human judgments, and standardized schema; intended for training compression models and computing per‑turn token usage.

---

