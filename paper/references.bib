@article{goyal2021recurrent,
  author = {Goyal, Anirudh and Binas, Jonathan and Blundell, Charles and Bengio, Yoshua and Botvinick, Matthew},
  title = {Recurrent Independent Mechanisms},
  journal = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year = {2021},
  pages = {1--18}
}

@article{chevalier2023compressed,
  author = {Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi},
  title = {Adapting Language Models to Compress Contexts},
  journal = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2023},
  pages = {1--15}
}

@article{wingate2022tokensqueeze,
  author = {Wingate, David and Shoeybi, Mohammad and Catanzaro, Bryan},
  title = {Compressing Context to Enhance Inference Computational Efficiency of Large Language Models},
  journal = {arXiv preprint arXiv:2211.01462},
  year = {2022}
}

@article{vaswani2017attention,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  title = {Attention is All You Need},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2017},
  volume = {30},
  pages = {5998--6008}
}

@inproceedings{brown2020language,
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  title = {Language Models are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2020},
  volume = {33},
  pages = {1877--1901}
}

@article{park2023generative,
  author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie J and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  title = {Generative Agents: Interactive Simulacra of Human Behavior},
  journal = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
  year = {2023},
  pages = {1--22}
}

@article{qian2023communicative,
  author = {Qian, Chen and Cong, Xin and Yang, Cheng and Chen, Weize and Su, Yusheng and Xu, Juyuan and Liu, Zhiyuan and Sun, Maosong},
  title = {Communicative Agents for Software Development},
  journal = {arXiv preprint arXiv:2307.07924},
  year = {2023}
}

@inproceedings{moura2021lean,
  author = {de Moura, Leonardo and Ullrich, Sebastian},
  title = {The Lean 4 Theorem Prover and Programming Language},
  booktitle = {International Conference on Automated Deduction (CADE)},
  year = {2021},
  pages = {625--635}
}

@article{zheng2023lmsys,
  author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  title = {Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena},
  journal = {arXiv preprint arXiv:2306.05685},
  year = {2023}
}

@article{kingma2014adam,
  author = {Kingma, Diederik P and Ba, Jimmy},
  title = {Adam: A Method for Stochastic Optimization},
  journal = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year = {2015}
}

@article{loshchilov2017adamw,
  author = {Loshchilov, Ilya and Hutter, Frank},
  title = {Decoupled Weight Decay Regularization},
  journal = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year = {2019}
}

@article{press2021lowrank,
  author = {Press, Ofir and Smith, Noah A and Lewis, Mike},
  title = {Shortformer: Better Language Modeling using Shorter Inputs},
  journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)},
  year = {2021},
  pages = {5493--5505}
}

@article{hu2021lora,
  author = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  title = {LoRA: Low-Rank Adaptation of Large Language Models},
  journal = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year = {2022}
}

@article{ainslie2023colt5,
  author = {Ainslie, Joshua and Lei, Tao and de Jong, Michiel and Ontan\~{o}n, Santiago and Brahma, Siddhartha and Zemlyanskiy, Yury and Uthus, David and Guu, Kelvin and Lee-Thorp, James and Sanghai, Sumit},
  title = {CoLT5: Faster Long-Range Transformers with Conditional Computation},
  journal = {arXiv preprint arXiv:2303.09752},
  year = {2023}
}

@inproceedings{zaheer2020bigbird,
  author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  title = {Big Bird: Transformers for Longer Sequences},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2020},
  volume = {33},
  pages = {17283--17297}
}
