# Sources

## 1. Performance-Preserving Compression for Reasoning LLMs (TokenSqueeze) - arXiv

**URL:** https://arxiv.org/abs/2511.13223

**Summary:** ArXiv abstract for TokenSqueeze, proposing a Long→Short training method to compress reasoning traces while preserving performance; used as the primary source for TokenSqueeze claims.

---

## 2. Performance-Preserving Compression for Reasoning LLMs - OpenReview PDF

**URL:** https://openreview.net/pdf?id=Wc1VZ2bVJn

**Summary:** OpenReview PDF of the TokenSqueeze submission; provides methodology details and experimental claims about token reductions with preserved reasoning accuracy.

---

## 3. Performance-Preserving Compression for Reasoning LLMs (TokenSqueeze) - arXiv PDF

**URL:** https://arxiv.org/pdf/2511.13223

**Summary:** Full arXiv PDF for TokenSqueeze offering technical details, experiments, and reported token‑savings; used to corroborate claims from the abstract/openreview.

---

## 4. CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling - arXiv

**URL:** https://arxiv.org/abs/2509.09199

**Summary:** ArXiv abstract for the Context Compression Framework (CCF), which proposes hierarchical latent compression to represent long contexts compactly.

---

## 5. CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling - arXiv PDF

**URL:** https://arxiv.org/pdf/2509.09199

**Summary:** Full arXiv PDF for CCF with architecture, hierarchical latent representation details, and experiments demonstrating context compression benefits.

---

## 6. Context Compression & Selective Expansion - Emergent Mind

**URL:** https://www.emergentmind.com/topics/context-compression-and-selective-expansion

**Summary:** Summary and discussion of context compression and selective expansion ideas; provides interpretation and critique of hierarchical latent compression approaches like CCF.

---

## 7. Recurrent Independent Mechanisms - arXiv

**URL:** https://arxiv.org/abs/1909.10893

**Summary:** Original RIMs paper proposing modular recurrent modules with sparse communication and selective activation; foundational architecture for sparse coordination.

---

## 8. Recurrent Independent Mechanisms - OpenReview forum

**URL:** https://openreview.net/forum?id=mLcmdlEUxy-

**Summary:** OpenReview entry for the RIMs work giving peer review context and additional technical discussion about sparse modular recurrence.

---

## 9. LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions - arXiv

**URL:** https://arxiv.org/pdf/2405.11106

**Summary:** A recent survey of LLM-based multi‑agent systems that discusses workflows and research gaps, used to corroborate broader statements about the scarcity of token‑budgeted coordination evaluations.

---

## 10. TRINITY: An Evolved LLM Coordinator - ResearchGate

**URL:** https://www.researchgate.net/publication/398357551_TRINITY_An_Evolved_LLM_Coordinator

**Summary:** A non‑peer-reviewed coordinator design (TRINITY) that exemplifies coordinator role‑assignment ideas and provides context on evolving coordinator architectures for multi‑LLM systems.

---

